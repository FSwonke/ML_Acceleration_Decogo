Title of Concept
----------------
ML Acceleration for decogo

1 Background
------------

Describe
* Project background: motivation of the concept
* status quo of our software
* refer to existing documentation, if applicable

- subproblem solving contributes most to the runtime of process

- accelerating subproblem solving offers great potential to decrease runtime

- solving subproblems is finding new columns which minimize reduced cost

- reduced cost/dual solution "or" reduced cost direction is required to find new inner points

- to calculate an inner point from direction an external solver is called (ipopt(NLP)/scip(MINLP))



2 Functional description and non-functional requirements
--------------------------------------------------------

�WHAT�-Part

Describe the goal
* what this concept shall achieve (new or changed functionality)
* what constraints must be fulfilled (e.g. performance aspects)
Goals shall be verifiable such they can be tested in the end. If the verifiability is not obvious (e.g. �the size of the network shall be reduced substantially�), please consider how measurability of the targets can be achieved. Details, however, do not have to be described here. It suffices to take them into account when the tests are described below.

- Implementation of a machine learning algorithm to accelerate Decogo. This is achieved by training a neuronal network to predict feasible (inner) points from directions.

In Detail the following steps have to be fulfilled:

- Implementation of storing training data (direction, point)

- Implementation of training a Neuronal Network 

- Implementation of applying the neuronal Network

- Implementation of anomaly detection

3 Design and implementation outline
-----------------------------------

�HOW shall it work�-Part

Describe
* Class design
* (Public) Interfaces of classes
* Outline of the implementation (in cases when the implementation approach is a critical part of the overall solution, e.g., when a new, non-trivial algorithm shall be introduced)
* refer to existing documentation, if applicable

- Class Design for training data

    *Name: SubSolverData
        using a dictionary for storage and list of tuples
        Methods:
            def add_data(block_id, direction, point)
            def get_size(block_id)

- Class Design for training the neuronal network
*parallel insatances of init class from pyomo_minlp_model (input_model)
*from scikit-learn use: MLPClassifier, StandardScaler, train_test_split

    *Name: SurrogateModel
        Parameters:
            classifiers: dict, elemen: 
            block_ids
            scaling_parameters: dict, elemen: mean, var 
            binary_index: dict, elemen: list of binary index in a block

        Methods:
            def __init__(block_ids, binary_index) todo
                #define the neuronal network
                #hidden layers
                #activation functions
                #solver
                     self.clf_batch = None
                
                # init define block_ids, binary indexes
                     self.block_ids ...
                     self.binary_index = binary_index

            def init_train(block_id, training_data) todo
                data from SubSolverData

                # read and input and output data
                    x, y
                    y = training_data[:, binary_index[block_id]]
                # preprocess input & output data -> scaled input output data
                    package, method called here 
                # initial training of the model -> fit self.clf_batch
                    self.clf_batch.fit(x, y)
                     
            def predict(block_id, direction) todo
            
            
     
            def test_init_train(vadidation_data, block_id)
            
            
            
            --------------------------------------------------------
            def update() 
                #accumulate more training data and update model

            def update_model_specifications()                



- Class Design for using the neuronal network
     
     - add SurrogateModel as parameter in
     class decogo.pyomo_minlp_model.input_model.PyomoSubProblems(sub_models, cuts, block_id, settings)
     
      --method:
        def ml_sub_solver_init_train()
            call SurrogateModel.init_train()
        def ml_sub_solver_test_init_train() 
        
     - add methods to
     class decogo.solver.refactory_colgen.RefactoryColGen(problem, settings, result)
        
        def ml_sub_solver_init()
            call SubProblems.ml_sub_solver_init_train() for all blocks
            
        def ml_sub_solver_test()
             call ml_sub_solver_test_init_train()
        
        -------------------------------
        def ml_sub_solver_update()
        
        
     
     - call ml_sub_solver_init() and ml_sub_solver_test() in solve() in the main iterations:
        ------------------------------------
          while True:
            self.result.main_iterations += 1
         -----------------------------------
         
     
     
     
     
     
     Class MLSubSolver
    *Name: "predictinnerpoint"
        Parameters:
            SurrogateModel

        Methods:
            def __init__()

            def predict(direction)
                SurrogateModel.predict()
                #uses the NN to predcit point
                
            def verfication()
                #evaulates if prediction made by the model is right/wrong

- class design for applying anomaly detection



4 Critical implementation details
---------------------------------

�HOW shall I do it�-Part

Describe details of the implementation.

This section can be kept very short or can even be empty if the implementation is considered to be clear enough for the developers because
* No major difficulties are expected and
* The developer is sufficiently familiar with the corresponding part of the software


- Class Design for training the neuronal network

      Implementation details of classifiers: todo



5 Test concept and execution summary
------------------------------------

This section must be updated and handed in for review by the developer when an activity is about to be published. In the end (after test execution), it shall describe all tests which shall be executed in addition to the mandatory publish tests and briefly summarize their results. The objective is not to have a detailed, reproducible test execution protocol but a brief indication about the nature and the level of detail of the tests such that the reviewer can judge whether the amount of testing is sufficient for the risk and complexity of the work package.
Additional or adapted unit tests
Brief summary suffices. Details referring to the implementation shall be described together with the concept for the production code.
Regression tests from the test suite
List standard regression test cases in our test suite. Explain if a more detailed analysis shall be done (beyond checking that the overall test passed).
Additional (manual) tests
Please list any other tests that need to be executed. Examples:
* Manually constructed small examples where the result has been precalculated.
* Detailed comparison (diff etc.) of two runs where a certain feature was switched on and off
* Detailed log file
* Performance tests on a dedicated environment to measure the speed up achieved by a feature.
* Profiling, Memory tests

6 Activity descriptions: coding, documentation and testing
----------------------------------------------------------

Devide the concept into activities.


7 plan & milestones: (2 days per week)

* implement surrogate model in sub-problem solving (init_train & update surrogate model & test)                    2021.11 (2 weeks)  

* implement ml-sub-solver with surrogate model (integration of surrogate model and other local sub-solver & test)  2021.12 (3 weeks)

* implement evaluator (anormly detection & test)                                                                   2021.12 - 2022.1 (2 weeks)

* tune overall ml-sub-solving (test & tune surrogate model/anormly detection model & extra test of algorithm )     2022.1 - 2022.2 (4 weeks)




